{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highland Lakes Storm Inflow Predictive Model (Preliminary)\n",
    "\n",
    "_Tyler Carstensen_\n",
    "\n",
    "##### Diagrams Courtesy   \n",
    "_Lower Colorado River Authority_  \n",
    "_United States Geological Survey_  \n",
    "_Modified Scraping script from Nathan Hilbert (Oak Ridge National Laboratory)_\n",
    "\n",
    "##### Technologies Used\n",
    "\n",
    " * psycopg2 (postgreSQL)\n",
    " * BeautifulSoup\n",
    " * Selenium (chrome driver)\n",
    " * Scipy Stack (Python Scientific Libraries)\n",
    " * Scikit-learn\n",
    " * Statsmodels\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Overview of the highland lakes system\n",
    "\n",
    "PICTURE OF LAKE TRAVIS   \n",
    "\n",
    "PICTURES OF DAMS  \n",
    "\n",
    "![highlandlakes](images/highlandlakes2008.jpg)\n",
    "![lakeprofile](images/lake_profile_no_data.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# May 2015 floods end the drought\n",
    "\n",
    "PICTURES OF FLOODED AUSTIN, WATER OVER LAMAR BRIDGE  (Intersting: Explain that this is JUST lady bird lake (or town lake for you austinites) flooding, which is a small lake. Upstream is lake austin, which has significantly more volume at a slightly higher elevation. Then lake travis, which has an exponentially higher volume at a much higher elevation, then buchanan which has a massive volume at a much higher elevation.\n",
    "\n",
    "Drought before/after pictures as seen on [Austin-American Statesmans Before and After Project](http://projects.statesman.com/news/lake-travis-levels/)\n",
    "\n",
    "## Lake Travis During Drought (2012)\n",
    "![RM620before](images/RM620before.png)\n",
    "\n",
    "## Lake Travis After Drought (2016)\n",
    "![RM620after](images/RM620after.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Many people (including our politicans and leaders) have forgotton the original mission of the dams - **Flood control**  \n",
    "The drought is over and now flooding is a renewed threat. Lake capacity acts as a 'buffer' against massive storm inflows from the surrounding basin that would typically flood Austin and the surrounding area.  \n",
    "\n",
    "Prior to engineered flood control, Travis county and the surrounding area were flooded frequently. Initial attempts to dam the colorado river in the early 1900s failed - City of Austin construction two dams, but both were destroyed by floods.\n",
    "\n",
    "Massive national civil engineering efforts in the middle of the 20th century were reponsible for the commissioning of the existing series of dams. They were constructed during a decades-long effort starting in 1935 and ending in 1951. The construction involved redirecting rivers and reshaping the terrain to influence the natural watersheds.\n",
    "\n",
    "**It is critical to know how much the lakes are going to fill after a storm**\n",
    "\n",
    "---\n",
    "\n",
    "How does weather affect the lakes?  \n",
    "PICTURE OF WEATHER STATIONS  \n",
    "![stormrainflow](images/stormrainflowusgs.gif)\n",
    "\n",
    "\n",
    "USGS Streamflow post storm image (Shows time delay between water flow and storm, but also how it can overlap)\n",
    "    (area under the curve after rainfall ends is a good indicator of what's going to happen. May not even need to catch the exact end of the rainfall)\n",
    "PICTURE OF LAKE INFLOWS POST WEATHER EVENT  \n",
    "(Weather itself is almost impossible to predict. It's difficult to say which watersheds recieved the rainfall. Streamflow gauges can give us additional data on which watersheds the storm affected.)  \n",
    "\n",
    "---\n",
    "Watersheds\n",
    "PICTURE OF WATERSHEDS\n",
    "Explain watershed (if you dump your drink anywhere in watershed x, it will always flow to a single point. If the watershed is draiing, it will always move to the edge of the watershed. Of course there are 'local minima', or small watersheds within these large watersheds - but the overall basin should typically drain most water in the same direction.\n",
    "\n",
    "Water that falls north of highland lake watersheds goes to the brazos river. The Lower Brazos is not controlled.\n",
    "\n",
    "The watersheds vary greatly in size - Lake Buchanan takes the majority of the rainfall.\n",
    "\n",
    "---\n",
    "\n",
    "Historical lake levels  \n",
    "CHART OF LAKE LEVEL HISTORY  \n",
    "PICTURES OF LAKE TRAVIS LOW AND HIGH  \n",
    " \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Civil Engineering Considerations on Dams  \n",
    "DIAGRAM OF DAM  \n",
    "Spillway to prevent structural damage  \n",
    "DAM FAILURE PICTURES  \n",
    "Austin flooding pictures\n",
    "\n",
    "---\n",
    "TYPICAL CIVIL ENGINEERING HYDROLOGY METHODS  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure\n",
    "\n",
    "#### _Model 1_  \n",
    "\n",
    "**Parameters**  \n",
    "* $h_{prior}$: Time window of precipitation prior to storm event (Hours)  \n",
    "* $h_{post}$: Time window after storm event (Hours)  \n",
    "* time_interval: Interval of time to slide the window, affects size of dataset (Hours)  \n",
    "* event_list: Specific chosen times to place the event window, if time_interval is not used (list of timestamps)  \n",
    "\n",
    "_Each feature will represent data for a specific time event. The windows will control the aggregated precipitation prior to the event and the aggregated lake inflows after the event. The events can be manually chosen (requires tediously reading precipitation logs) or chosen as arbitrary time intervals._    \n",
    "\n",
    "A smarter data analysis methodology would parse the features/responses and find the storm events and create a list of storm events. This would require additional work - but may be necessary if the model isn't working well.\n",
    "\n",
    "NOAA's NWS storm event lists - [NOAA's NWS storm event lists](http://www.spc.noaa.gov/climo/online/)    \n",
    "\n",
    "\n",
    "**Features**  \n",
    " * Rainfall in $h_{prior}$ at each chosen weather station  \n",
    " \n",
    " PICTURE OF AGGREGATING READINGS IN WEATHER STATION\n",
    " \n",
    " PICTURE OF WEATHER STATIONS (interesting: we don't care which watershed or lat/long of the stations. The machine learning algorithm should automatically fit coefficients to the features given the lake inflow response variable. It should group the weather stations by the affected watershed. Remember that rainfall on a single weather station may affect more than one watershed.)\n",
    " \n",
    "**Response Variable**  \n",
    " * Lake inflows at chosen lake in $h_{post}$ (cubic feet)  \n",
    " * BONUS Streamflow prediction (gonna be very hard...)  \n",
    "\n",
    "## Machine Learning Algorithms  \n",
    "\n",
    "**One model for each lake in the highland lakes chain**   \n",
    " * Linear Regression (multiple)  \n",
    "     * Regularization\n",
    "     * Forward-selection for collinear features\n",
    " * Randomforest (multiple)  \n",
    " \n",
    " * BONUS: Nueral Net. This would permit multiple response variables (all lakes) in one model. \n",
    "\n",
    "\n",
    "PAPER WILL BE A IPPYNB HTML FILE HOSTED ON GITHUB IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Conditioning and Pipeline Steps\n",
    "\n",
    "\n",
    "\n",
    "### 1. Determine which sensors to use by hydromet map.\n",
    "![watersheds_precipitation_stations](images/watersheds_precipitation_sidebar.png)\n",
    "\n",
    " * Check availability of that sensor on [LCRA's Chronhist database](http://hydromet.lcra.org/chronhist.aspx)\n",
    "     * This is a *manual Labor* step that takes time due to the nature of the site\n",
    "     * The history site only allows you to pull 180 days of data at a time\n",
    "     * Used an old scraper from Oak Ridge National Laboratory as a framework to build my scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of Sensors (Site Name, Site Number)\n",
    "\n",
    "# Inks, LBJ, Marble Falls and Austin are static pass-through lakes\n",
    "\n",
    "\n",
    "# lakelevel_sites = [\n",
    "#                    (\"Buchanan Dam\", 1995), #Lake Buchanan\n",
    "#                    (\"Inks Dam\", 1999),\n",
    "#                    (\"Lake LBJ at 2900 Bridge\", 2699),\n",
    "#                    (\"Wirtz Dam\", 2958), #Lake LBJ\n",
    "#                    (\"Starcke Dam\", 2999),\n",
    "#                    (\"Mansfield Dam\", 3963), #Lake Travis\n",
    "#                    (\"Tom Miller Dam\", 3999) #Lake Austin\n",
    "#                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each lake level site will be an independent model for lake inflows. They will share the same features, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Code Stubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gauge_list(url='http://hydromet.lcra.org/chronhist.aspx'):\n",
    "    \"\"\"Returns a list of gauge values/names.\n",
    "    \n",
    "    INPUT:\n",
    "        string  (url)  | URL of the site\n",
    "        \n",
    "    OUTPUT:\n",
    "        list           | list of tuples containing strings in format (gauge number, gauge name)\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    dl1 = soup.find(id='DropDownList1')\n",
    "    allgauges = dl1.find_all(\"option\")\n",
    "\n",
    "    gaugevalues = []\n",
    "    gaugenames = []\n",
    "\n",
    "    for gauge in allgauges:\n",
    "        gaugevalues.append(gauge.get('value'))\n",
    "        gaugenames.append(gauge.contents[0])\n",
    "\n",
    "\n",
    "    return zip(gaugevalues, gaugenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauge Value: 2992\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-7a4b84bb0057>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msensor_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "for gaugevalue, gaugename in zip(gaugevalues, gaugenames):\n",
    "    driver.get(\"http://hydromet.lcra.org/chronhist.aspx\")\n",
    "    select = Select(driver.find_element_by_name('DropDownList1'))\n",
    "    select.select_by_value(gaugevalue)\n",
    "    print \"Gauge Value: {}\".format(gaugevalue)\n",
    "    \n",
    "    # Get the new options for sensors\n",
    "    select_sensor = Select(driver.find_element_by_name('DropDownList2'))\n",
    "    sensor_options = select_sensor.options\n",
    "    \n",
    "    for option in sensor_options:\n",
    "        option.click()\n",
    "        sensor_value = option.get_attribute(\"value\")\n",
    "        sensor_name = option.text\n",
    "        print \"Sensor Value: {}\\t\\t Sensor Name: {}\".format(sensor_value, sensor_name)\n",
    "        sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Will store data in dataframes based on sensor type, by gauge\n",
    "        sensormap = zip(sensor_value, sensor_name)\n",
    "        \n",
    "        \n",
    "# WHEN FINISHED, TRY BREAKING INTO SUBFUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensormap = set(sensormap)\n",
    "sensormap = list(sensormap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gauge_drop_down.find_elements_by_tag_name(\"DropDownList1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://hydromet.lcra.org/chronhist.aspx\")\n",
    "select = Select(driver.find_element_by_name('DropDownList1'))\n",
    "options = select.options\n",
    "firstoption = options[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Backbone Creek at Marble Falls'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstoption.text # name of element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT STEP: verify all stations on chronhist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA: Draw storm curves (rain vs lake levels or streamflow) after scraping data\n",
    "\n",
    "Models: Try a model that discards non-precipitation events. Also try a model that includes precipitation events.\n",
    "\n",
    "Try a model for lake level delta, and also for raw lake level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOST DATASET ON KAGGLE TO OPTIMIZE ML?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
