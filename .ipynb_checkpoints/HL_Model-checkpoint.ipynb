{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highland Lakes Storm Inflow Predictive Model\n",
    "\n",
    "![highlandlakes](images/highlandlakes2008.jpg)\n",
    "\n",
    "--- \n",
    "\n",
    "## Summary\n",
    "\n",
    "A predictive machine learning model to estimate the lake inflows after major storms in the Austin, Texas area.\n",
    "\n",
    "The highland lakes are managed by the Lower Colorado River Authority (LCRA) and protect the downstream urban population living in the floodplain. A major multi-decade drought shifted the public focus from flood control to water conservation in the greater Austin area. In 2015, a major storm ended the drought and refilled the lakes.\n",
    "\n",
    "The highland lake levels are frequently reported by the local news media and monitored by ordinary Austinites and Texans in the surrounding area. It is not uncommon to hear area residents speaking about how the lakes are doing.\n",
    "\n",
    "The lakes are important to many area residents for multiple reasons:\n",
    "* Recreational (boating)\n",
    "* Financial (business water use)\n",
    "* Lawn-care and other homeowner restrictions (drought conditions limit water use)  \n",
    "\n",
    "\n",
    "This model intends to predict lake inflows during storm periods using a 'pure data' approach and minimal hydrological knowledge.\n",
    "    * Features: Rain gauge data\n",
    "    * Response: Lake Inflow\n",
    "\n",
    "Limitations to keep the project under 5 days:\n",
    "    * No Lake Outflow data\n",
    "    * No soil moisture data\n",
    "    * Not all gauges/sensors were accounted for\n",
    "\n",
    "_Note: This project is intended to demonstrate using machine learning methods on a relevant dataset. LCRA's employees maintain much more useful models for their flood operations._\n",
    "\n",
    "---\n",
    "\n",
    "##### Diagrams Courtesy   \n",
    "_Lower Colorado River Authority_  \n",
    "_United States Geological Survey_  \n",
    "_Modified Scraping script from Nathan Hilbert (Oak Ridge National Laboratory)_\n",
    "\n",
    "##### Technologies Used\n",
    "\n",
    " * psycopg2 (postgreSQL)\n",
    " * BeautifulSoup\n",
    " * Selenium (chrome driver)\n",
    " * Scipy Stack (Python Scientific Libraries)\n",
    " * Scikit-learn\n",
    " * Statsmodels\n",
    " * Amazon AWS Relational Database Server\n",
    " \n",
    " \n",
    " ##### Time Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Task|Portion of Time Spent|\n",
    "|---|---|\n",
    "|Selenium Scraper|60%|\n",
    "|Feature Engineering|30%|\n",
    "|Statistical Analysis|7%|\n",
    "|Model Fitting|3%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Overview of the highland lakes system\n",
    "\n",
    "\n",
    "\n",
    "![lakeprofile](images/lake_profile_no_data.png)\n",
    "\n",
    "This project focused on the inflows to Lake Travis (Mansfield Dam) as the response variable.\n",
    "\n",
    "---\n",
    "\n",
    "# May 2015 floods end the drought\n",
    "\n",
    "Drought before/after pictures as seen on [Austin-American Statesmans Before and After Project](http://projects.statesman.com/news/lake-travis-levels/)\n",
    "\n",
    "#### Lake Travis During Drought (2012)\n",
    "![RM620before](images/RM620before.png)\n",
    "\n",
    "#### Lake Travis After Drought (2016)\n",
    "![RM620after](images/RM620after.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# How does weather affect the lakes?  \n",
    "Rainfall lands in the surrounding basin feeding the lakes. The initial rainfall is absorbed into the ground. During and (typically) after the storm, the water collects in streams. The streams gradually fill the lake after the storm.\n",
    "![stormrainflow](images/stormrainflowusgs.gif)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# Sensors\n",
    "\n",
    "Rain gauges are placed throughout the watersheds to monitor where rain is falling during a storm. This helps predict which lakes will receive inflows - buying time to begin releasing water from the various dams as needed to create flood capacity.\n",
    "![watershedsensors](images/watersheds_precipitation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauge Groups Utilized\n",
    "\n",
    " 1. Austin Watershed  \n",
    "    * 3999, Tom Miller Dam\n",
    "    * 4594, Driftwood 4 SSE\n",
    "    * 3991, Jollyville 2 SW\n",
    " 2. Lake Travis Watershed (Primary objective)\n",
    "    * 3963, Mansfield Dam\n",
    "    * 3948, Lakeway 2 E\n",
    "    * 3448, Blanco 5 NNE\n",
    "    * 3237, Harper 4 SSW\n",
    "    * 3015, Burnet 1 WSW\n",
    "    * 2634, Cherokee 4 SSE\n",
    "    * 2348, Menard 12 SSE\n",
    "    * 2140, Sonora 14 SE\n",
    "    * 2248, Rocksprings 12 NE\n",
    " 3. Lake Buchanan Watershed\n",
    "    * 1995, Buchanan Dam\n",
    "    * 1921, Lometa 2 WNW\n",
    "    * 1405, Eldorado 2 E (Missing readings prior to 2006-12-19)\n",
    "    * 1090, Millersview 7 WSW (Missing readings prior to 2006-12-19)\n",
    "    * 1307, Clyde 6 S\n",
    "    * 1197, Rochelle 5 NNW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "Web scraping was performed since only a subset of the data was needed and to avoid taking time from employees working in LCRA's organization.\n",
    "\n",
    "Web scraping was conducted using: \n",
    " * Selenium webscraper driven by Chrome (and PhantomJS)\n",
    " * BeautifulSoup4 (For parsing html)\n",
    " * An AWS-hosted Relational Database\n",
    " * Rotating proxy servers\n",
    " \n",
    "---\n",
    "\n",
    "### Scraper Methodology\n",
    "\n",
    "Due to the javascript-heavy nature of the site, the selenium scraper needed to be able to fill out a dynamically-changing form using drop-boxes and text-entry for dates.\n",
    "\n",
    "    1. Organize a list of all possible gauges\n",
    "    2. Select a gauge from a drop down box (Causing the page to regenerate with new options)\n",
    "          * Parse the new options available for sensors for each gauge     \n",
    "    3. Cycle through sensors for each gauge\n",
    "    4. Enter two date fields with in a 180-day range (maximum per site guidelines)     \n",
    "          * Cycle through dates until no more data was available\n",
    "    5. Submit a request with the fields filled in\n",
    "    6. Parse a javascript table, reading headers and the columns of data\n",
    "    7. Format the data and submit it to the AWS-hosted SQL server.\n",
    "          * Limited by batches of 999 entries.\n",
    "    \n",
    "The final web scraper implementation required 300 lines of code including 1 class and 13 methods. The SQL interface required 200 lines of code including 1 class and 8 methods.\n",
    "\n",
    "Due to very off-and-on scraping, **very verbose logs** of scraper activity were generated. When a scraper failed, it would be evident where to pick back up. Functions were written to restart the scraper at a specific state.\n",
    "\n",
    "Example of scraper logs:\n",
    "```\n",
    "----------------------------------------------------------------------\n",
    "----------------------------------------------------------------------\n",
    "                                                                      \n",
    "selected \t- DropDownList1\n",
    "clicked \t-            1405  Eldorado 2 E  \n",
    "----------------------------------------------------------------------\n",
    "----------------------------------------------------------------------\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "selected \t- DropDownList2\n",
    "clicked \t- PC              Rainfall at Site\n",
    "----------------------------------------------------------------------\n",
    "selected \t- Date2\n",
    "entered \t- End Date        06/01/2017     \n",
    "selected \t- Date1\n",
    "entered \t- Start Date      12/04/2016     \n",
    "clicked \t- Button1\n",
    "parsed \t\t- tbody\n",
    "access time\t- 06/01/2017 12:49:12\n",
    "inserted \t- 34194 records\n",
    "\n",
    "selected \t- Date2\n",
    "entered \t- End Date        12/03/2016     \n",
    "selected \t- Date1\n",
    "entered \t- Start Date      06/07/2016     \n",
    "clicked \t- Button1\n",
    "parsed \t\t- tbody\n",
    "access time\t- 06/01/2017 12:50:09\n",
    "inserted \t- 33806 records\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storm Event Aggregation\n",
    " 1. Get the maximum precipitation value from any sensor for all observation times (SQL)\n",
    " 2. Remove any values below zero (negative rainfall...) (Use DataFrame)\n",
    " 3. Create a time series that uses trailing and leading averages to 'smooth' out the rainfall data (numpy, loops)\n",
    " 4. Programming algorithm to capture where the moving average 'crosses' the storm threshold to define storm events  \n",
    " \n",
    "Rain data was very sparse and often involved very jittery values. A sliding average and sliding 'sum' were used over all rain gauges to discover when storms were in the system.     \n",
    " \n",
    "Storms and the corresponding moving average were checked by printing a long 'tape' style time series over many, many, graphs. This was used to visualize the moving average against precipitation readings. Storm events were defined by setting a 'moving average storm threshold'. When the moving average crossed the threshold, a storm began or ended.  \n",
    "\n",
    "\n",
    "**Ticker Tape**:  \n",
    "Precipitation (blue)    \n",
    "Moving sum (orange)\n",
    "  \n",
    "![stormevents](images/raintape2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "The vast majority of time was spent transforming very granular and high-variance sensor data into useable features. A series of queries, loops, and meta-feature creation were necessary to adapt the data to the correct format.\n",
    "\n",
    "# Data cleaning\n",
    "Minimal data cleaning was necessary.\n",
    "1. Broken HTML table values that were replaced with zeroes\n",
    "2. Some negative rainfall inches values were removed\n",
    "\n",
    "# Feature Engineering\n",
    "Data transformation was very comprehensive. The features needed to be grouped and transformed from a series of short (but variable) time intervals into discrete storm events.\n",
    "\n",
    "No data existed on start/stop times for the storm events, so they needed to be defined using the precipitation moving average.\n",
    "\n",
    "Also, lake level data was not useful for predicting inflows. A lake level volume was needed to create lake volume deltas. \n",
    "\n",
    "\n",
    "![datajourney](images/transform of data.png)\n",
    "\n",
    "## Lake Geometry\n",
    "Lake level is a poor response variable since lakes typically contain more volume at higher levels. Fortunately, LIDAR/Bathymetric surveys of the lakes were performed by the Texas Water Development board. From this data, tables were provided to map lake levels to volume.  \n",
    "Elevation-Volume curves were used to estimate the volume at various levels.\n",
    "\n",
    "Below: Snippets from the 2008 Lake Buchanan Survey\n",
    "\n",
    "![lake surveys](images/bathymetry.png)\n",
    "![level-volumecurve](images/buchanan voluem curves.png)\n",
    "\n",
    "### How big was the Data?\n",
    "|Type|Count|Description|\n",
    "|---|:---|:---|\n",
    "|Raw gauges/sensors Dataset|250,000,000+|The slow SQL queries made it evident to select a subset of gauges|\n",
    "|Selected sub-dataset|30,792,945|Selection of 20 'key' gauges based on topographic location|\n",
    "|Filtered sensors|1,651,708|SQL interface query to pull appropriate data out of sensors|\n",
    "|+ Sliding meta-Feature|1,651,708|Feature Engineering - 'Sliding Sum'/'Sliding Average' for each value|\n",
    "|+ Storm meta-feature|727|Depending on threshold, custom-algorithm storm start-end list|\n",
    "|+ Lake Volume Delta|727|Created using 2007/2008 LIDAR/bathymetric survey data to estimate volume by lake level|\n",
    "|Engineered Features|727|Aggregate(Sum/Min/Max/Delta) Queried Data|\n",
    "|Filter Sensor Data|436|Removed rows with missing sensor data. Some sensors were not installed early on|\n",
    "|Filter Major Storms|407|(Regression only) Largest storms were removed from dataset to improve accuracy|\n",
    "\n",
    "\n",
    "---\n",
    "**Custom Algorithms that were created**:\n",
    " * Storm event classification algorithm with threshold slider\n",
    " * Algorithm for moving sum with variable leading/trailing distance\n",
    "\n",
    "SQL queries were automated and were performed in batches of 999 items (limit for PSQL server is 1000). Queries were used in tandem with DataFrame manipultion to construct the features.\n",
    "\n",
    "The largest challenge with the feature engineering was speeding up the feature construction algorithms. Big-O optimization for the loops, as well as balancing SQL Query/DataFrame loop size was key to speeding up the process. The code for the aggregation algorithms is inside the sql_class.py and storm_pipeline.py modules.  \n",
    "\n",
    "**An example would be a SQL query to get the minimum and maximum lake level for each storm event:**  \n",
    "\n",
    "```python\n",
    "    def get_max_min_lakes(self, start_time, end_time):\n",
    "\n",
    "        cur = self.conn.cursor()\n",
    "        q = \"\"\"\n",
    "        SELECT DISTINCT gauge, MIN(value), MAX(value)\n",
    "        FROM hydromet\n",
    "        WHERE (collection_time BETWEEN %s AND %s) AND\n",
    "              (sensor = 'Lake Level (ft above MSL)')\n",
    "        GROUP BY gauge\n",
    "        \"\"\"\n",
    "        cur.execute(q, (start_time, end_time))\n",
    "\n",
    "        data = cur.fetchall()\n",
    "        cur.close()\n",
    "        return data\n",
    "```\n",
    "\n",
    "Many similar queries were used to get data from the server to 'speed up' the feature reduction step without needing to download the entire 30mm+ row dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA was used for 'Reality' checks and (sometimes painfully obvious) observations about the data\n",
    "\n",
    "The data was visually inspected to ensure that the feature transformation process was successful and generated useful information.\n",
    "\n",
    "---\n",
    "\n",
    "#### Dam Volumes throughout historical storm events:\n",
    "![maxhist](images/eda_max_hist.png)\n",
    "\n",
    "\n",
    "\n",
    " * The distribution of lake levels is **bimodal**, which is expected in a system that is designed to manage massive storm inflows, and also conserve water during droughts.\n",
    " \n",
    "#### Inflows throughout historical storm events:\n",
    "\n",
    "![inflowhist](images/eda_inflow_hist.png)\n",
    "\n",
    "Most inflows are minor, with a few very major inflows that characterize the 'refilling' of the lakes during massive storm events.  \n",
    "\n",
    "**Below, inflows are filtered to remove small storms:**\n",
    "\n",
    "![inflowhist](images/eda_inflowgreater_hist.png)\n",
    "\n",
    "Clearly, several very large storms occured that caused 'outlier' inflows.\n",
    "\n",
    "#### Storm event durations\n",
    "\n",
    "![durations](images/stormduration.png)\n",
    "\n",
    "Most of the storms were clustered around the 10 hour mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "The first model attempted was a linear regression model. This was performed to try to draw inferences about the model and see if the watershed rainfall features were affecting the appropriate inflow response variable.\n",
    "\n",
    "### Collinearity\n",
    "\n",
    "**High Variance Inflation Factors** were discovered for many of the rain gauges. VIFs above 5 were removed.\n",
    "\n",
    "![viff](images/vifs3.png)\n",
    "\n",
    "### Features chosen\n",
    "\n",
    "An initial regression was fit to look for features that were not contributing to the Mansfield Dam inflow response variable. This was characterized by large P-values.\n",
    "\n",
    "![initial](images/initialR22.png)\n",
    "\n",
    "\n",
    "This information, combined with high VIFs, resulted in many features being filtered out.  \n",
    "\n",
    "The resulting features:\n",
    "![regressionfeatures](images/LinearRegressionFeatures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Removal\n",
    "\n",
    "By setting the inflow threshold to 25,000 acre-feet, regression was much more successful. The only drawback being that the model will only predict smaller stormflows.  \n",
    "\n",
    "The sample size was reduced from 436 to 407 by removing very large storm events.\n",
    "\n",
    "\n",
    "### Multivariate Regression Model\n",
    "\n",
    "Full Dataset Coefficient of Determination $ R^2 = 0.591 $\n",
    "\n",
    "|Parameter|Value|\n",
    "|---|---|---|\n",
    "|Type|Least Squares|\n",
    "|Normalization|False|\n",
    "|Regularization|Ridge|\n",
    "|Outliers Removed|Above 25,000 acre-feet inflow|\n",
    "|Features Removed|F-Statistic P-value > 0.05|\n",
    "\n",
    "Ridge regression resulted in a small increase to final score\n",
    "\n",
    "\n",
    "### Regression Results \n",
    "\n",
    "![OLSRESULTS](images/OLSResults2.png)\n",
    "\n",
    "**Condition Number:** The condition number is not terrible - indicating some, but not extreme collinearity. This condition number reflects the collinearity _after_ removing the features with high VIF.\n",
    "\n",
    "\n",
    "### Linear Regression Performance\n",
    "\n",
    "### MAE vs MSE\n",
    "\n",
    "Mean absolute error was chosen as the preferred scoring due to the variable sizes in the response variable. Mean squared error was rejected because it would overly penalize large errors (which are likely as storms increase in size)\n",
    "\n",
    "#### KFolds\n",
    "\n",
    "Data was split using KFolds into 5 sets of train/test data.  \n",
    "\n",
    "\n",
    "> Average storm Inflow for Mansfield Dam   \n",
    "> 5397 acre-feet    \n",
    "\n",
    "> Standard deviation of storm inflows for Mansfield Dam   \n",
    "> 5142 acre-feet    \n",
    "\n",
    "> Average Ridge Regression Mean Absolute Error (MAE) for each storm prediction   \n",
    "> **3161 acre-feet** \n",
    "\n",
    "The performance is not terrible given that the standard deviation is already fairly high. However, a regression model may not be the perfect model for this dataset.\n",
    "\n",
    "\n",
    "## Concerns\n",
    "\n",
    "A linear regression model works best on typical storms - but underperforms on major ones. This is because there is limited data about large storms (hence the terms '10 year storm' and '100 year storm').  \n",
    "\n",
    "The linear regression model is hamstringed by collinearity and low flexibility. A randomforest can overcome alot of the limitations that it suffers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Regression Model\n",
    "\n",
    "A randomforest grid search was conducted to fit a random forest regression model.\n",
    "\n",
    "#### Outlier Removal\n",
    "\n",
    "The large storms did have a serious effect on the output of the RandomForest as well. The same criteria learned in linear regression was applied to the RandomForest data.\n",
    "\n",
    "The inflow threshold was set to 25,000 acre-feet.\n",
    "\n",
    "The sample size was reduced from 436 to 407 by removing very large storm events.\n",
    "\n",
    "### Features used\n",
    "\n",
    "Unlike linear regression - multicollinearity is not an issue for a randomforest regressor. All available features were used. More features were used in the RandomForest model including features from other watersheds.  \n",
    "\n",
    "Random Forest Features:   \n",
    "\n",
    "![randomforest](images/RandomForestFeatures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Performance\n",
    "\n",
    "#### KFolds\n",
    "\n",
    "Data was split using KFolds into 5 sets of train/test data.  \n",
    "\n",
    "This was chosen over validation using OOB sample to ensure no leakage.\n",
    "\n",
    "\n",
    "> Average storm Inflow for Mansfield Dam   \n",
    "> 2,228 acre-feet    \n",
    "\n",
    "> Median storm Inflow for Mansfield Dam\n",
    "> 1,900 acre-feet\n",
    "\n",
    "> Standard deviation of storm inflows for Mansfield Dam   \n",
    "> 1,204 acre-feet    \n",
    "\n",
    "> Average Ridge Regression Mean Absolute Error (MAE) for each storm prediction   \n",
    "> **979 acre-feet** \n",
    "\n",
    "Given the extreme variability, small sample size, and high variance, RandomForest performed well. \n",
    "\n",
    "The following settings were used to acquire the best performing model.\n",
    "\n",
    "```python\n",
    "#  {'warm_start': False, 'oob_score': False, 'n_jobs': -1, 'verbose': 0,\n",
    "# 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 5, 'n_estimators': 50,\n",
    "# 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mae', 'random_state': 1,\n",
    "# 'min_impurity_split': 1e-07, 'max_features': 'auto', 'max_depth': 3}\n",
    "```\n",
    "##### Performance Validation\n",
    "\n",
    "Several attempts were made to modify the performance of the model.\n",
    "\n",
    "> 1) Large storms were removed (> 4,319 acre-feet inflow)  \n",
    "     * The large storms are typically rare and there are not many data points for them.\n",
    "     \n",
    "> 2) Small storms were filtered out (< 1000 acre-feet inflow)\n",
    "     * Result: Model accuracy did not change much.\n",
    "     * This means that the 'small values' were not causing an artifially low MAE.\n",
    "\n",
    "#### Feature Importances\n",
    "\n",
    "**88.4%** of the feature importance came from the four gauges direclty in the Lake Travis Watershed. This means that the gauges in the watershed were directly affecting the reading, with a little bit of effect from gauges in the neighboring (Lake LBJ) watershed.  \n",
    "\n",
    "The feature importances are exactly what you would expect, given the natural relationship.\n",
    "\n",
    "Orange circles denote the chosen 4 gauges in the watershed:\n",
    "\n",
    "![topfis](images/topfis.png)\n",
    "\n",
    "\n",
    "```python\n",
    "Feature Importances\n",
    "[('Menard 12 SSE', 0.00804764),\n",
    " ('Sonora 14 SE', 0.00956749),\n",
    " ('Rochelle 5 NNW', 0.01008855),\n",
    " ('Lakeway 2 E', 0.02777929),\n",
    " ('Jollyville 2 SW', 0.02802943),\n",
    " ('Millersview 7 WSW', 0.0281413),\n",
    " ('Rocksprings 12 NE', 0.03081275),\n",
    " ('Clyde 6 S', 0.03524796),\n",
    " ('Lometa 2 WNW', 0.03830201),\n",
    " ('Eldorado 2 E', 0.05155255),\n",
    " ('Driftwood 4 SSE', 0.06015059),\n",
    " ('Cherokee 4 SSE', 0.06429295),\n",
    " ('Harper 4 SSW', 0.10045381),\n",
    " ('Blanco 5 NNE', 0.20920885),\n",
    " ('Burnet 1 WSW', 0.29832482)]\n",
    "``` \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project could be greatly expanded to include many more gauges. The high variability of weather and natural stormflows makes any inflow prediction modelling difficult. The goal was to complete the project in under 5 days - and the majority of time was spent dealing with the web scraper and data transformation.\n",
    "\n",
    "Ways to expand on this project:\n",
    "\n",
    "    * Include all gauges\n",
    "    * Incoporate data prior to 2006\n",
    "    * Try varying the storm definition threshold\n",
    "    * Include many more variables including floodgate operations and soil moisture"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
